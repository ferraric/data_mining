Our first thought was to implement k means in the mapper. But then we realized we had no smart way of deciding which centers to keep in the reducer. Then we tried to implement online k-means in the reducer, which worked fine but we didn’t quite crack the baseline. We thought we needed a better initialization (we first did it randomly). We tried picking data points at random, farthest point heuristic, but nothing brought us great improvement. Then we tried kmeans ++, the problem with that was it was way too slow, so we found an approach called kmeans ||, that supposedly is way faster than kmeans++ and also gives you good initial centers. That brought us a little improvement, but it wasn’t enought. Lastly we discovered that scipy offers an implementation of kmeans. So we ran that with our kmeans|| initialization and that got us over the hard baseline. With that approach we don’t do any work in the mapper which makes using the mapreduce framework pointless but as we tried many fancy approaches in previous projects and then later found out that a simpler approach worked better, we do not feel guilty for presenting a “simple” solution.