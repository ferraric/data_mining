We started off trying to implement online k-means in the mapper to obtain our centers and then just output them in the reducer. The performance wasn't too good, and the question was how to get or aggregate the centers from the mapper to 200 centers. We moved away from this solution and then tried to implement the online k-means algorithm with randomized initial centers in the reducer. This proved to be a quite good solution, but it only got very close to the easy baseline. Playing with the stepsize wasn't improving the solution enough, so we looked at different initialization algorithms in clustering problems. We found out about k-means++ (which we later looked at in the lecture), but after implementing this, we quickly realized that this takes way too long, since it takes k passes over the entire dataset for the shortest distances, and with a  growing number of obtained centers each iteration took longer and longer, so we needed a quicker initialization method. After some research, we found out about the k-means|| initialization, which, using an oversampling factor, gives a lot more initial centers in much less iterations. Using this initialization method and the online k-means algorithm, we saw some more improvements, but it still wasn't enough for the easy baseline, so we ended up using the same initialization but the k-means implementation of SciPy, which gave us the final solution that beats the hard baseline. It seems like their implementation is quite a lot better.
