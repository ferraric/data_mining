For this project we started out by implementing an epsilon greedy strategy. It performed really bad and ran for a long time (maybe there were also some implementation flaws). 
From there we went on to the linear ucb algorithm presented in the lecture. That performed ok after some parameter tuning. A really crucial detail for our success (not only for lin ucb but also later for hybrid ucb) was realizing that the reward could be thought of as a learning rate parameter and therefore could also be manipulated. That got us close to the easy baseline but no further. 
Our final and ultimately successful approach was with hybrid ucb. It took some time to implement and understand what it really does. At first the results were very poor, both performance and execution time wise. After again tuning the parameters the performance was improved to above the hard baseline. We got the execution time down by trying to cache as much information as we could in order to save computation. With that we got it to finish within the time limit.
The biggest takeaway for me was that parameter tuning can be everything for this type of problem. Our hybrid ucb more than doubled its CTR after choosing the right parameters.